{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/yue/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yue/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yue/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic NLP with NLTK\n",
    "from nltk.corpus import brown\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "with open(\"en.json\") as json_data:\n",
    "    en_json = json.load(json_data)\n",
    "\n",
    "stopwords_json_en = set(en_json)\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. \n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv', index_col = 0)\n",
    "def rebuild(data):\n",
    "    data_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        name = data.at[i,'name']\n",
    "        desc = data.at[i,'description']\n",
    "        if type(desc) is float:\n",
    "            if type(data.at[i,'label_1']) is not float:\n",
    "                labels = data.at[i,'label_1'] + ' ' + data.at[i,'label_2'] + ' ' + data.at[i,'label_3']\n",
    "                desc = labels + ' ' + name\n",
    "            else:\n",
    "                desc = name\n",
    "        dic = {'name':name,'content': desc}\n",
    "        data_list.append(dic)\n",
    "    df = pd.DataFrame.from_dict(data_list)\n",
    "    return df\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "def to_count_vec(string):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    analysis = [word for word in lemmatize_sent(string) \n",
    "       if word not in stoplist_combined\n",
    "       and not word.isdigit() ]\n",
    "    return analysis\n",
    "\n",
    "def my_cluster(dataframe, num = 100):\n",
    "    tmp = dataframe['content'].values.astype('U')\n",
    "    # from nltk import sent_tokenize, word_tokenize\n",
    "    # count_vect = CountVectorizer(stop_words=stoplist_combined, tokenizer=word_tokenize)\n",
    "    vectorizer = CountVectorizer(analyzer=to_count_vec)     # same as above\n",
    "    X = vectorizer.fit_transform(tmp)\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    \n",
    "    km = KMeans(n_clusters=num)\n",
    "    km.fit(tfidf)\n",
    "    clusters = km.labels_.tolist()\n",
    "#     answer={'answer_body':document, 'cluster':clusters} \n",
    "    frame=pd.DataFrame({'name':dataframe['name'],'content': tmp, 'cluster': clusters}, columns=['name','content','cluster'])\n",
    "    return frame.sort_values(by='cluster', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = rebuild(data)\n",
    "    g_frame = my_cluster(df,175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15015</td>\n",
       "      <td>whole earth sugar substitute sweetener 60g</td>\n",
       "      <td>Pantry Baking Sugar &amp; Sweeteners whole earth s...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16621</td>\n",
       "      <td>selleys sugar soap sugar soap</td>\n",
       "      <td>Household Cleaning Glass &amp; Multipurpose Cleane...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3086</td>\n",
       "      <td>double d sweets sugar free butter candy</td>\n",
       "      <td>double d sugar relieve butter candy drops with...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16725</td>\n",
       "      <td>atkins endulge nutrition bar chocolate mint</td>\n",
       "      <td>low carb milk chocolate bar cocoa solids dry w...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3741</td>\n",
       "      <td>chelsea caster sugar</td>\n",
       "      <td>very fine white sugar crystals make caster sug...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11701</td>\n",
       "      <td>havana plunger super deluxe</td>\n",
       "      <td>of deluxe blend a coffees left with love an el...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11720</td>\n",
       "      <td>havana coffee beans super deluxe</td>\n",
       "      <td>a deluxe commingle of coffees roasted with lov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11728</td>\n",
       "      <td>hummingbird harvest organic coffee beans seaso...</td>\n",
       "      <td>whole beans coffee tree fair swap organic coff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11768</td>\n",
       "      <td>hummingbird nectar organic plunger grind coffee</td>\n",
       "      <td>coffee plunger grind fair trade organic coffee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18106</td>\n",
       "      <td>hummingbird oomph! organic espresso grind coffee</td>\n",
       "      <td>espresso coffee grind fair trade constituent c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19711 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "15015         whole earth sugar substitute sweetener 60g   \n",
       "16621                      selleys sugar soap sugar soap   \n",
       "3086             double d sweets sugar free butter candy   \n",
       "16725        atkins endulge nutrition bar chocolate mint   \n",
       "3741                               chelsea caster sugar    \n",
       "...                                                  ...   \n",
       "11701                        havana plunger super deluxe   \n",
       "11720                   havana coffee beans super deluxe   \n",
       "11728  hummingbird harvest organic coffee beans seaso...   \n",
       "11768    hummingbird nectar organic plunger grind coffee   \n",
       "18106   hummingbird oomph! organic espresso grind coffee   \n",
       "\n",
       "                                                 content  cluster  \n",
       "15015  Pantry Baking Sugar & Sweeteners whole earth s...      174  \n",
       "16621  Household Cleaning Glass & Multipurpose Cleane...      174  \n",
       "3086   double d sugar relieve butter candy drops with...      174  \n",
       "16725  low carb milk chocolate bar cocoa solids dry w...      174  \n",
       "3741   very fine white sugar crystals make caster sug...      174  \n",
       "...                                                  ...      ...  \n",
       "11701  of deluxe blend a coffees left with love an el...        0  \n",
       "11720  a deluxe commingle of coffees roasted with lov...        0  \n",
       "11728  whole beans coffee tree fair swap organic coff...        0  \n",
       "11768  coffee plunger grind fair trade organic coffee...        0  \n",
       "18106  espresso coffee grind fair trade constituent c...        0  \n",
       "\n",
       "[19711 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
